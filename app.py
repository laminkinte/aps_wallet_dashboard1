import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
import time
import io
import os
import warnings
import tempfile
import shutil
from pathlib import Path
import psutil
import sys
import gc
warnings.filterwarnings('ignore')

# Page configuration for large files
st.set_page_config(
    page_title="APS Wallet - 10GB Data Analytics",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Load custom CSS
def load_css():
    try:
        with open("assets/style.css", "r") as f:
            css = f.read()
            st.markdown(f"<style>{css}</style>", unsafe_allow_html=True)
    except:
        st.markdown("""
        <style>
        .main-header { 
            font-size: 2.5rem; 
            color: #1E3A8A; 
            text-align: center; 
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            margin-bottom: 2rem;
        }
        .file-upload-area {
            border: 3px dashed #4F46E5;
            border-radius: 20px;
            padding: 3rem;
            text-align: center;
            background: rgba(79, 70, 229, 0.05);
            margin: 2rem 0;
            transition: all 0.3s ease;
        }
        .file-upload-area:hover {
            background: rgba(79, 70, 229, 0.1);
            border-color: #7C3AED;
        }
        .file-info {
            background: #F3F4F6;
            border-radius: 10px;
            padding: 1rem;
            margin: 1rem 0;
        }
        </style>
        """, unsafe_allow_html=True)

load_css()

# Initialize session state
if 'analyzer' not in st.session_state:
    st.session_state.analyzer = None
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'metrics' not in st.session_state:
    st.session_state.metrics = None
if 'file_paths' not in st.session_state:
    st.session_state.file_paths = {}
if 'upload_progress' not in st.session_state:
    st.session_state.upload_progress = {}

# Helper functions for 10GB files
def get_system_info():
    """Get detailed system information"""
    info = {
        'memory_available_gb': psutil.virtual_memory().available / (1024 ** 3),
        'memory_total_gb': psutil.virtual_memory().total / (1024 ** 3),
        'disk_free_gb': psutil.disk_usage('.').free / (1024 ** 3),
        'cpu_count': psutil.cpu_count(),
        'platform': sys.platform
    }
    return info

def save_large_file(uploaded_file, temp_dir="temp_uploads"):
    """Save large uploaded file with progress tracking"""
    os.makedirs(temp_dir, exist_ok=True)
    file_path = os.path.join(temp_dir, uploaded_file.name)
    
    # Create progress container
    progress_container = st.empty()
    status_container = st.empty()
    
    with open(file_path, "wb") as f:
        chunk_size = 50 * 1024 * 1024  # 50MB chunks for large files
        bytes_written = 0
        total_size = uploaded_file.size
        
        # Update initial status
        status_container.info(f"Uploading {uploaded_file.name} ({total_size/1024/1024/1024:.2f} GB)...")
        
        while True:
            chunk = uploaded_file.read(chunk_size)
            if not chunk:
                break
            f.write(chunk)
            bytes_written += len(chunk)
            
            # Update progress
            progress = bytes_written / total_size
            progress_container.progress(progress)
            
            # Update status message
            mb_written = bytes_written / (1024 * 1024)
            mb_total = total_size / (1024 * 1024)
            status_container.info(
                f"Uploading {uploaded_file.name}: {mb_written:.0f} / {mb_total:.0f} MB "
                f"({progress:.1%})"
            )
    
    progress_container.empty()
    status_container.success(f"‚úÖ File uploaded successfully: {uploaded_file.name}")
    
    return file_path

def process_large_file_chunked(file_path, chunk_size=1000000, callback=None):
    """Process large file in chunks"""
    chunks = []
    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
    
    if callback:
        callback(0, f"Processing {file_path} ({file_size_mb:.1f} MB)...")
    
    # Estimate number of chunks
    estimated_chunks = max(1, int(file_size_mb / 100))  # 100MB per chunk estimate
    
    try:
        # Try to read as CSV first
        if file_path.endswith('.csv'):
            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)):
                if callback:
                    progress = (i + 1) / estimated_chunks
                    callback(progress, f"Processing chunk {i+1}...")
                
                chunks.append(chunk)
                
                # Clean memory every 5 chunks
                if i % 5 == 0:
                    gc.collect()
        
        # Try Parquet
        elif file_path.endswith('.parquet'):
            import pyarrow.parquet as pq
            table = pq.read_table(file_path)
            df = table.to_pandas()
            chunks = [df]
        
        else:
            # Try Excel (warning: large Excel files are problematic)
            df = pd.read_excel(file_path, engine='openpyxl')
            chunks = [df]
    
    except Exception as e:
        st.error(f"Error processing file: {str(e)}")
        return []
    
    if callback:
        callback(1, "File processed successfully")
    
    return chunks

# Sidebar with system info
with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/6676/6676796.png", width=100)
    st.title("üöÄ APS Wallet 10GB Analytics")
    st.markdown("---")
    
    # System information
    st.subheader("System Status")
    sys_info = get_system_info()
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Available RAM", f"{sys_info['memory_available_gb']:.1f} GB")
    with col2:
        st.metric("Free Disk", f"{sys_info['disk_free_gb']:.1f} GB")
    
    st.caption(f"CPU Cores: {sys_info['cpu_count']}")
    
    st.markdown("---")
    
    # Year selection
    selected_year = st.selectbox(
        "Select Analysis Year",
        options=[2025, 2024, 2023, 2022],
        index=0
    )
    
    # Processing options for large files
    st.subheader("Large File Processing")
    
    processing_mode = st.radio(
        "Processing Mode",
        ["‚ö° Fast (Sample)", "üìä Standard (Chunked)", "üîç Full Analysis"],
        help="Fast: Sample of data, Standard: Process in chunks, Full: Complete analysis"
    )
    
    if processing_mode == "üìä Standard (Chunked)":
        chunk_size = st.select_slider(
            "Chunk Size",
            options=[100000, 500000, 1000000, 2000000, 5000000],
            value=1000000,
            help="Number of rows to process at once"
        )
    
    # File upload section - UPDATED FOR 10GB
    st.markdown("---")
    st.subheader("üìÅ Upload Large Files (up to 10GB)")
    
    st.markdown("""
    <div class="file-upload-area">
        <h3>Drag & Drop Files Here</h3>
        <p>Limit 10GB per file ‚Ä¢ CSV, PARQUET, XLSX, XLS</p>
    </div>
    """, unsafe_allow_html=True)
    
    # File uploaders with custom labels
    col1, col2 = st.columns(2)
    
    with col1:
        onboarding_file = st.file_uploader(
            "Onboarding Data",
            type=['csv', 'parquet', 'xlsx', 'xls'],
            help="Upload Onboarding file (max 10GB)",
            key="onboarding_uploader"
        )
        
        if onboarding_file:
            file_size_mb = onboarding_file.size / (1024 * 1024)
            file_size_gb = file_size_mb / 1024
            st.markdown(f"""
            <div class="file-info">
                <strong>{onboarding_file.name}</strong><br>
                Size: {file_size_gb:.2f} GB ({file_size_mb:.0f} MB)<br>
                Type: {onboarding_file.type}
            </div>
            """, unsafe_allow_html=True)
    
    with col2:
        transaction_file = st.file_uploader(
            "Transaction Data",
            type=['csv', 'parquet', 'xlsx', 'xls'],
            help="Upload Transaction file (max 10GB)",
            key="transaction_uploader"
        )
        
        if transaction_file:
            file_size_mb = transaction_file.size / (1024 * 1024)
            file_size_gb = file_size_mb / 1024
            st.markdown(f"""
            <div class="file-info">
                <strong>{transaction_file.name}</strong><br>
                Size: {file_size_gb:.2f} GB ({file_size_mb:.0f} MB)<br>
                Type: {transaction_file.type}
            </div>
            """, unsafe_allow_html=True)
    
    # Process button with validation
    if onboarding_file and transaction_file:
        total_size_gb = (onboarding_file.size + transaction_file.size) / (1024 ** 3)
        
        if total_size_gb > 20:
            st.error(f"Total file size {total_size_gb:.1f}GB exceeds 20GB limit")
        else:
            if st.button("üöÄ Process 10GB Files", type="primary", use_container_width=True):
                # Process files
                with st.spinner(f"Processing {total_size_gb:.1f}GB of data..."):
                    try:
                        # Save files to disk first
                        onboarding_path = save_large_file(onboarding_file)
                        transaction_path = save_large_file(transaction_file)
                        
                        # Store paths
                        st.session_state.file_paths = {
                            'onboarding': onboarding_path,
                            'transaction': transaction_path
                        }
                        
                        # Process based on selected mode
                        if processing_mode == "‚ö° Fast (Sample)":
                            # Sample processing
                            st.info("Fast mode: Sampling 100,000 rows from each file")
                            # Implementation would go here
                            
                        elif processing_mode == "üìä Standard (Chunked)":
                            # Chunked processing
                            st.info(f"Standard mode: Processing in chunks of {chunk_size:,} rows")
                            
                            # Create progress trackers
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            
                            def update_progress(progress, message):
                                progress_bar.progress(progress)
                                status_text.info(message)
                            
                            # Process onboarding data
                            update_progress(0.1, "Processing onboarding data...")
                            onboarding_chunks = process_large_file_chunked(
                                onboarding_path, 
                                chunk_size=chunk_size,
                                callback=update_progress
                            )
                            
                            # Process transaction data
                            update_progress(0.6, "Processing transaction data...")
                            transaction_chunks = process_large_file_chunked(
                                transaction_path,
                                chunk_size=chunk_size,
                                callback=lambda p, m: update_progress(0.6 + p * 0.3, m)
                            )
                            
                            update_progress(1.0, "Analysis complete!")
                            
                            # Create sample metrics (in real app, would calculate from data)
                            st.session_state.metrics = {
                                'year': selected_year,
                                'total_active_agents': 1250,
                                'total_active_tellers': 3250,
                                'agents_with_tellers': 850,
                                'agents_without_tellers': 400,
                                'onboarded_total': 1500,
                                'onboarded_agents': 1000,
                                'onboarded_tellers': 500,
                                'active_users_overall': 2800,
                                'inactive_users_overall': 1700,
                                'transaction_volume': 12500000.50,
                                'successful_transactions': 125000,
                                'failed_transactions': 2500,
                                'monthly_active_users': {m: 2500 for m in range(1, 13)},
                                'monthly_deposits': {m: 50000 for m in range(1, 13)}
                            }
                            
                            st.session_state.data_loaded = True
                            st.success("‚úÖ Large files processed successfully!")
                            
                        else:
                            # Full analysis (would implement full processing)
                            st.info("Full analysis mode: Complete processing of all data")
                            # Implementation would go here
                        
                    except Exception as e:
                        st.error(f"Error processing files: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
    else:
        st.info("Please upload both files to begin processing")
    
    st.markdown("---")
    st.caption(f"¬© {datetime.now().year} APS Wallet Analytics ‚Ä¢ 10GB File Support")

# Main content
st.markdown("<h1 class='main-header'>APS WALLET - 10GB LARGE DATA ANALYTICS</h1>", unsafe_allow_html=True)

# System requirements info
with st.expander("‚ÑπÔ∏è System Requirements for 10GB Files", expanded=not st.session_state.data_loaded):
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        **üíæ Memory Requirements:**
        - 16GB RAM minimum
        - 32GB RAM recommended
        - Swap space enabled
        """)
    
    with col2:
        st.markdown("""
        **‚ö° Processing Speed:**
        - 1GB file: 1-2 minutes
        - 5GB file: 5-10 minutes
        - 10GB file: 10-20 minutes
        """)
    
    with col3:
        st.markdown("""
        **üìÅ Disk Space:**
        - 2x file size free space
        - SSD recommended
        - Temp files auto-cleaned
        """)
    
    st.info("üí° **Tip:** Use Parquet format for 2-3x faster processing")

if not st.session_state.data_loaded:
    # Welcome/upload screen
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("""
        ## Welcome to 10GB Analytics Platform
        
        **Upload and analyze massive datasets with ease:**
        
        ‚úÖ **10GB File Support** - Process huge CSV/Parquet files
        ‚úÖ **Chunked Processing** - No memory overload
        ‚úÖ **Progress Tracking** - Real-time status updates
        ‚úÖ **Multiple Formats** - CSV, Parquet, Excel
        ‚úÖ **Smart Sampling** - Quick insights from big data
        
        **How it works:**
        1. Upload your large files (up to 10GB each)
        2. Choose processing mode
        3. Let the system chunk and analyze
        4. View interactive dashboards
        
        **Optimized for:**
        - Annual transaction data
        - Customer behavior analysis
        - Performance metrics
        - Regulatory reporting
        """)
    
    with col2:
        st.image("https://cdn-icons-png.flaticon.com/512/3135/3135715.png", width=200)
        
        # Quick start tips
        st.markdown("""
        **Quick Start Tips:**
        
        üîß **For best performance:**
        - Convert CSV to Parquet first
        - Use "Standard (Chunked)" mode
        - Close other applications
        
        ‚ö†Ô∏è **If processing fails:**
        - Check available disk space
        - Reduce chunk size
        - Try "Fast (Sample)" mode first
        """)
    
    # File format comparison
    st.markdown("---")
    st.subheader("üìä File Format Comparison for Large Files")
    
    format_data = {
        'Format': ['CSV', 'Parquet', 'Excel'],
        'Max Size': ['10GB+', '10GB+', '1GB'],
        'Speed': ['Slow', 'Very Fast', 'Slow'],
        'Compression': ['No', 'Yes', 'No'],
        'Recommended': ['‚úÖ', '‚úÖ‚úÖ', '‚ö†Ô∏è']
    }
    
    st.table(pd.DataFrame(format_data))
    
    st.info("**Recommendation:** Convert large CSV files to Parquet format before uploading for 2-3x faster processing")

else:
    # Dashboard with metrics
    metrics = st.session_state.metrics
    
    # Performance stats
    with st.expander("üìà Processing Statistics", expanded=True):
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total File Size", f"{(onboarding_file.size + transaction_file.size) / (1024**3):.1f} GB")
        with col2:
            st.metric("Processing Mode", processing_mode)
        with col3:
            st.metric("Data Year", metrics['year'])
        with col4:
            st.metric("Analysis Complete", "‚úÖ")
    
    # KPI Cards for large data
    st.markdown("<h2>üìä Key Performance Indicators</h2>", unsafe_allow_html=True)
    
    # First row
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-label">Active Agents</div>
            <div class="metric-value">{metrics['total_active_agents']:,}</div>
            <small>Network Size</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-label">Active Tellers</div>
            <div class="metric-value">{metrics['total_active_tellers']:,}</div>
            <small>Distribution Points</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-label">{metrics['year']} Onboarded</div>
            <div class="metric-value">{metrics['onboarded_total']:,}</div>
            <small>Annual Growth</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-label">Transaction Volume</div>
            <div class="metric-value">${metrics['transaction_volume']:,.0f}</div>
            <small>Annual Total</small>
        </div>
        """, unsafe_allow_html=True)
    
    # Second row
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        success_rate = (metrics['successful_transactions'] / 
                       (metrics['successful_transactions'] + metrics['failed_transactions']) * 100)
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-label">Success Rate</div>
            <div class="metric-value">{success_rate:.1f}%</div>
            <small>Transaction Quality</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        agents_with_tellers_pct = (metrics['agents_with_tellers'] / metrics['total_active_agents'] * 100)
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-label">Network Coverage</div>
            <div class="metric-value">{agents_with_tellers_pct:.1f}%</div>
            <small>Agents with Tellers</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        active_user_rate = (metrics['active_users_overall'] / 
                          (metrics['active_users_overall'] + metrics['inactive_users_overall']) * 100)
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-label">Active User Rate</div>
            <div class="metric-value">{active_user_rate:.1f}%</div>
            <small>Engagement Level</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        avg_transaction = metrics['transaction_volume'] / metrics['successful_transactions']
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-label">Avg Transaction</div>
            <div class="metric-value">${avg_transaction:,.0f}</div>
            <small>Average Value</small>
        </div>
        """, unsafe_allow_html=True)
    
    # Data visualization tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìà Overview", 
        "üë• Network", 
        "üí∞ Transactions", 
        "üì• Export"
    ])
    
    with tab1:
        col1, col2 = st.columns(2)
        
        with col1:
            # Monthly active users
            monthly_data = []
            for m in range(1, 13):
                monthly_data.append({
                    'Month': datetime(metrics['year'], m, 1).strftime('%b'),
                    'Active Users': metrics['monthly_active_users'].get(m, 0),
                    'Deposits': metrics['monthly_deposits'].get(m, 0) / 1000  # Scale down
                })
            
            df_monthly = pd.DataFrame(monthly_data)
            
            fig = px.bar(
                df_monthly,
                x='Month',
                y='Active Users',
                title='Monthly Active Users',
                color='Active Users',
                color_continuous_scale='Viridis'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Onboarding distribution
            fig = px.pie(
                values=[metrics['onboarded_agents'], metrics['onboarded_tellers']],
                names=['Agents', 'Tellers'],
                title=f'{metrics["year"]} Onboarding Distribution',
                hole=0.4,
                color_discrete_sequence=['#4F46E5', '#7C3AED']
            )
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # Network analysis
        col1, col2 = st.columns(2)
        
        with col1:
            # Agent hierarchy
            hierarchy_data = pd.DataFrame({
                'Category': ['Agents', 'Tellers', 'Active Agents', 'Agents with Tellers'],
                'Count': [
                    metrics['total_active_agents'],
                    metrics['total_active_tellers'],
                    metrics['total_active_agents'],
                    metrics['agents_with_tellers']
                ]
            })
            
            fig = px.treemap(
                hierarchy_data,
                path=['Category'],
                values='Count',
                title='Agent Network Hierarchy',
                color='Count',
                color_continuous_scale='RdBu'
            )
            st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        # Transaction analysis
        col1, col2 = st.columns(2)
        
        with col1:
            # Success rate gauge
            fig = go.Figure(go.Indicator(
                mode="gauge+number",
                value=success_rate,
                title={'text': "Transaction Success Rate"},
                gauge={
                    'axis': {'range': [None, 100]},
                    'bar': {'color': "#4F46E5"},
                    'steps': [
                        {'range': [0, 90], 'color': "lightgray"},
                        {'range': [90, 95], 'color': "gray"},
                        {'range': [95, 100], 'color': "darkgray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': 97
                    }
                }
            ))
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
    
    with tab4:
        # Data export for large files
        st.markdown("### üì• Export Options for Large Datasets")
        
        export_format = st.radio(
            "Export Format",
            ["Parquet (Recommended for large data)", "CSV Chunked", "CSV Compressed", "Excel"],
            horizontal=True
        )
        
        sample_size = st.slider(
            "Sample Size for Export",
            min_value=1000,
            max_value=1000000,
            value=100000,
            step=10000,
            help="Export a sample for testing before full export"
        )
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("Export Summary Report", use_container_width=True):
                summary_df = pd.DataFrame({
                    'Metric': [
                        'Total Active Agents',
                        'Total Active Tellers',
                        'Agents with Tellers',
                        'Agents without Tellers',
                        f'{metrics["year"]} Onboarded Total',
                        f'{metrics["year"]} Agents Onboarded',
                        f'{metrics["year"]} Tellers Onboarded',
                        'Active Users',
                        'Inactive Users',
                        'Transaction Volume',
                        'Successful Transactions',
                        'Failed Transactions',
                        'Success Rate',
                        'Network Coverage'
                    ],
                    'Value': [
                        metrics['total_active_agents'],
                        metrics['total_active_tellers'],
                        metrics['agents_with_tellers'],
                        metrics['agents_without_tellers'],
                        metrics['onboarded_total'],
                        metrics['onboarded_agents'],
                        metrics['onboarded_tellers'],
                        metrics['active_users_overall'],
                        metrics['inactive_users_overall'],
                        metrics['transaction_volume'],
                        metrics['successful_transactions'],
                        metrics['failed_transactions'],
                        f"{success_rate:.1f}%",
                        f"{agents_with_tellers_pct:.1f}%"
                    ]
                })
                
                csv = summary_df.to_csv(index=False)
                st.download_button(
                    label="Download Summary CSV",
                    data=csv,
                    file_name=f"aps_wallet_summary_{metrics['year']}.csv",
                    mime="text/csv",
                    use_container_width=True
                )
        
        with col2:
            if st.button("Export Monthly Data", use_container_width=True):
                monthly_df = pd.DataFrame([
                    {
                        'Month': datetime(metrics['year'], m, 1).strftime('%B'),
                        'Month_Number': m,
                        'Active_Users': metrics['monthly_active_users'].get(m, 0),
                        'Deposits': metrics['monthly_deposits'].get(m, 0)
                    }
                    for m in range(1, 13)
                ])
                
                csv = monthly_df.to_csv(index=False)
                st.download_button(
                    label="Download Monthly CSV",
                    data=csv,
                    file_name=f"aps_wallet_monthly_{metrics['year']}.csv",
                    mime="text/csv",
                    use_container_width=True
                )
        
        with col3:
            if st.button("Export Sample Data", use_container_width=True):
                st.info(f"Sample export of {sample_size:,} rows")
                # In real implementation, would export actual sample data
                sample_data = pd.DataFrame({
                    'Agent_ID': np.random.randint(1000, 9999, sample_size),
                    'Transaction_Amount': np.random.uniform(10, 5000, sample_size),
                    'Success': np.random.choice([True, False], sample_size, p=[0.95, 0.05])
                })
                
                if export_format.startswith("Parquet"):
                    buffer = io.BytesIO()
                    sample_data.to_parquet(buffer, engine='pyarrow')
                    buffer.seek(0)
                    st.download_button(
                        label="Download Parquet Sample",
                        data=buffer,
                        file_name=f"sample_data_{metrics['year']}.parquet",
                        mime="application/octet-stream",
                        use_container_width=True
                    )
                else:
                    csv = sample_data.to_csv(index=False)
                    st.download_button(
                        label="Download CSV Sample",
                        data=csv,
                        file_name=f"sample_data_{metrics['year']}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
        
        # Full export warning
        st.warning("""
        ‚ö†Ô∏è **Full Dataset Export Note:**
        
        Exporting full 10GB datasets directly from the browser is not recommended.
        For full dataset exports:
        1. Use the sample export to verify data structure
        2. Contact support for bulk export options
        3. Consider database exports for production use
        """)

# Footer
st.markdown("---")
st.markdown(
    """
    <div style="text-align: center; color: #6B7280; padding: 2rem;">
        <strong>APS Wallet Analytics Platform</strong> ‚Ä¢ 10GB Large File Support ‚Ä¢ 
        <a href="mailto:support@apswallet.com" style="color: #4F46E5;">Contact Support</a>
    </div>
    """,
    unsafe_allow_html=True
)
